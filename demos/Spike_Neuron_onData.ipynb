{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the model and algorithm of Tutorial--Van der Pol's dynamics with spike train observations.ipynb on the data in the folder Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# .py\n",
    "sys.path.append(\"..\")\n",
    "from inference import GaussMarkovLagrange\n",
    "from likelihoods import PointProcess\n",
    "from mappings import AffineMapping\n",
    "from transition import FixedPointSparseGP, SparseGP\n",
    "from kernels import RBF\n",
    "from linkfunctions import Exp\n",
    "from models import PointProcessGPSDEmodel, GPSDE\n",
    "sys.path.append(\"../Data\")\n",
    "from Load_plot_data import load_neuron_data, roster_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and modify its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "ids,times=load_neuron_data('../Data/Cellline1_Date190328_Chip2135.npz')\n",
    "N=np.max(ids) # Number of neurons\n",
    "trLen=np.array([np.max(times)]) #Tmax must be an array to have one for each trial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the files is of the form\n",
    "( id : neurons ids,\n",
    "times : times of spike )\n",
    "\n",
    "For the model we need a list of array where array i contains the times at which neuron i had a spike"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe using unique and some tests that the ids are all the numbers between 0 and 1016 except 131 and 899. We suppose that they are not present beacause they didn't have a spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n"
     ]
    }
   ],
   "source": [
    "Yspike=[]\n",
    "for i in range(N):\n",
    "    Yspike.append(times[ids==i])\n",
    "print(len(Yspike))\n",
    "\n",
    "#For now there is only one trial, to change if we want to define trials\n",
    "Yspike=[Yspike]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choices for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of variables\n",
    "dtgrid = 0.004 # discretisation for solving ODEs => maybe try to use one from the data instead\n",
    "xDim = 2 # two latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of model components\n",
    "link = Exp() # exponential link function (to define nonlinearity in likelihood )\n",
    "like = PointProcess(Yspike, link, trLen, dtstep=dtgrid, nLeg=100) # point process likelihood \n",
    "kern = RBF(xDim) # RBF kernel\n",
    "\n",
    "#To initialize the Affine mapping from latent to observations\n",
    "C = 2.*np.random.rand(xDim,N) * np.random.choice([-1,1],size=(xDim,N)) \n",
    "d = 0.1*np.random.randn(1,N)\n",
    "outputMapping = AffineMapping(torch.tensor(C), torch.tensor(d)) # affine output mapping, used inn the forward of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\logiciel\\miniconda\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Choose the inducing points and transition function\n",
    "\n",
    "# generater inducing point locations on a 2D grid (for sparse gp)\n",
    "xmin, xmax = -2., 2.\n",
    "Zs1, Zs2 = torch.meshgrid([torch.linspace(xmin,xmax,5), torch.linspace(xmin,xmax,5)])\n",
    "numZ = 25\n",
    "Zs = torch.cat((Zs1.reshape(-1,1),Zs2.reshape(-1,1)),dim=1)\n",
    "\n",
    "transfunc = SparseGP(kern, Zs) # choose sparse GP as a transition function\n",
    "transfunc.q_mu.data = torch.randn(transfunc.q_mu.size()).type(torch.float64) # random initialisation for inducing point posterior mean\n",
    "\n",
    "# uncomment to instead condition on fixed points\n",
    "#Zs_fx = torch.tensor([0., 0.]).view(-1,xDim)\n",
    "# transition function conditioned on fixed point\n",
    "#transfunc = SparseGP(kern, Zs, Zs_fx) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "\n",
    "# build point procces generative model for continuous time spike-time observations\n",
    "model = PointProcessGPSDEmodel(xDim, transfunc, outputMapping, like, nLeg=100)\n",
    "\n",
    "# assemble inference algorithm\n",
    "inference = GaussMarkovLagrange(xDim, trLen, learningRate=1, dtstep=dtgrid)\n",
    "\n",
    "# create GPSDE model object (final object)\n",
    "myGPSDE = GPSDE(model, inference)\n",
    "\n",
    "# fix inducing points on chosen grid\n",
    "myGPSDE.model.transfunc.Zs.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how long it takes to run the algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estep number of step for inference, mstepiter for learning update, niter for everything"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to do only one step for every part for the all dataset (niter=1,estepIter=1,mstepiter=1) => more than 130 min and it was stopped in inference update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\kernels.py:320: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
      "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
      "X = torch.solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve(A, B) (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:766.)\n",
      "  scaled_diffs, _ = torch.solve(mu_x2_new_diffs.transpose(-1, -2), lengthscales_new_squared.unsqueeze(-3))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m myGPSDE\u001b[39m.\u001b[39;49mvariationalEM(niter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,eStepIter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, mStepIter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:44\u001b[0m, in \u001b[0;36mvariationalEM\u001b[1;34m(self, niter, eStepIter, mStepIter)\u001b[0m\n\u001b[0;32m     41\u001b[0m time0\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(niter):\n\u001b[0;32m     43\u001b[0m     \u001b[39m# run inference across all trials\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minference_update(eStepIter)\n\u001b[0;32m     45\u001b[0m     time1\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     46\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minference time=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(time1\u001b[39m-\u001b[39mtime0))\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:58\u001b[0m, in \u001b[0;36mlearning_update\u001b[1;34m(self, niterM)\u001b[0m\n\u001b[0;32m     55\u001b[0m     time3=time.time()\n\u001b[0;32m     56\u001b[0m     print(\"update time\"+str(time3-time2))\n\u001b[1;32m---> 58\u001b[0m     # print some output and store cost function values\n\u001b[0;32m     59\u001b[0m     self.callback(i, final_ell, final_kld, final_prior_trans)\n\u001b[0;32m     61\u001b[0m # final inference pass\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:264\u001b[0m, in \u001b[0;36mcollectInferenceResults\u001b[1;34m(self, inference)\u001b[0m\n\u001b[0;32m    261\u001b[0m nquad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlike\u001b[39m.\u001b[39mxxLeg\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]  \u001b[39m# number of quadrature nodes\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# create empty tensors to store predicted means and variances across all trials\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m m_qu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(inference\u001b[39m.\u001b[39mnTrials, nquad, \u001b[39m1\u001b[39m, inference\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n\u001b[0;32m    265\u001b[0m S_qu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(inference\u001b[39m.\u001b[39mnTrials, nquad, inference\u001b[39m.\u001b[39mnLatent, inference\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n\u001b[0;32m    266\u001b[0m A_qu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(inference\u001b[39m.\u001b[39mnTrials, nquad, inference\u001b[39m.\u001b[39mnLatent, inference\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\inference.py:164\u001b[0m, in \u001b[0;36mGaussMarkovLagrange.predict_marginals\u001b[1;34m(self, idx, t)\u001b[0m\n\u001b[0;32m    162\u001b[0m S_linp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(T, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnLatent, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n\u001b[0;32m    163\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m--> 164\u001b[0m     m_linp[i, :, :] \u001b[39m=\u001b[39m linInterp(t[i], m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_grid[idx])\n\u001b[0;32m    165\u001b[0m     S_linp[i, :, :] \u001b[39m=\u001b[39m linInterp(t[i], S, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_grid[idx])\n\u001b[0;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m m_linp, S_linp\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\utils.py:345\u001b[0m, in \u001b[0;36mlinInterp\u001b[1;34m(t, A, t_grid, asnumpy)\u001b[0m\n\u001b[0;32m    343\u001b[0m     Astop \u001b[39m=\u001b[39m A[\u001b[39m0\u001b[39m, ]\n\u001b[0;32m    344\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[1;32m--> 345\u001b[0m \u001b[39melif\u001b[39;00m t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m t_grid\u001b[39m.\u001b[39;49mmax():  \u001b[39m# use only last grid point\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     Astart \u001b[39m=\u001b[39m A[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ]\n\u001b[0;32m    347\u001b[0m     Astop \u001b[39m=\u001b[39m A[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myGPSDE.variationalEM(niter=1,eStepIter=1, mStepIter=1) #estep number of step for inference, mstepiter for learning update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see on a smaller subset of data which part of the algorithm takes longer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 3 neurons with 1 general iteration, 1 iteration for inference and 1 iteration for learning:\n",
    "\n",
    "inference time = 313 s, learning time= 100 s ,update time =0s, Total : 13 min\n",
    "\n",
    "Details inference :\n",
    "forward= 36s, grad= 0.1 s, backward=275s, update= 1.8\n",
    "\n",
    "Details backward : kullberk div=231 ,Euler=44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "N_neurons=3 #only 3 neurons\n",
    "Y_spike_small=[Yspike[0][:N_neurons]]\n",
    "print(len(Y_spike_small[0]))\n",
    "\n",
    "like_small = PointProcess(Y_spike_small, link, trLen, dtstep=dtgrid, nLeg=100) # point process likelihood \n",
    "C_small = 2.*np.random.rand(xDim,N_neurons) * np.random.choice([-1,1],size=(xDim,N_neurons)) \n",
    "d_small = 0.1*np.random.randn(1,N_neurons)\n",
    "outputMapping_small = AffineMapping(torch.tensor(C_small), torch.tensor(d_small)) # affine output mapping, used inn the forward of the model\n",
    "model_small = PointProcessGPSDEmodel(xDim, transfunc, outputMapping_small, like_small, nLeg=100)\n",
    "# create GPSDE model object (final object)\n",
    "GPSDEsmall = GPSDE(model_small, inference)\n",
    "# fix inducing points on chosen grid\n",
    "GPSDEsmall.model.transfunc.Zs.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeforward36.850502729415894\n",
      "time grad0.1466076374053955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\kernels.py:320: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
      "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
      "X = torch.solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve(A, B) (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:766.)\n",
      "  scaled_diffs, _ = torch.solve(mu_x2_new_diffs.transpose(-1, -2), lengthscales_new_squared.unsqueeze(-3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time backward kull 230.78594946861267\n",
      "Time backard Euler44.26887631416321\n",
      "time backward275.0947186946869\n",
      "time update infer1.8450682163238525\n",
      "inference time=313.93988943099976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\utils.py:140: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A).transpose(-2, -1).conj().\n",
      "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1285.)\n",
      "  L = torch.cholesky(M)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning time=99.3671042919159\n",
      "update time0.0\n",
      "-------------------------------------------------------\n",
      "iter   objective    log-like      kl-div     f-prior\n",
      "-------------------------------------------------------\n",
      "   0    3980.048   -2704.684     377.148    -898.216\n",
      "timeforward38.5817506313324\n",
      "time grad0.18250226974487305\n",
      "time backward kull 360.7610158920288\n",
      "Time backard Euler59.15757179260254\n",
      "time backward420.00235867500305\n",
      "time update infer2.480560779571533\n"
     ]
    }
   ],
   "source": [
    "GPSDEsmall.variationalEM(niter=1,eStepIter=1, mStepIter=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
