{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the model and algorithm of Tutorial--Van der Pol's dynamics with spike train observations.ipynb on the data in the folder Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# .py\n",
    "sys.path.append(\"..\")\n",
    "from inference import GaussMarkovLagrange\n",
    "from likelihoods import PointProcess\n",
    "from mappings import AffineMapping\n",
    "from transition import FixedPointSparseGP, SparseGP\n",
    "from kernels import RBF\n",
    "from linkfunctions import Exp\n",
    "from models import PointProcessGPSDEmodel, GPSDE\n",
    "sys.path.append(\"../Data\")\n",
    "from Load_plot_data import load_neuron_data, roster_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and modify its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "ids,times=load_neuron_data('../Data/Cellline1_Date190328_Chip2135.npz')\n",
    "N=np.max(ids) # Number of neurons\n",
    "trLen=np.array([np.max(times)]) #Tmax must be an array to have one for each trial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the files is of the form\n",
    "( id : neurons ids,\n",
    "times : times of spike )\n",
    "\n",
    "For the model we need a list of array where array i contains the times at which neuron i had a spike"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe using unique and some tests that the ids are all the numbers between 0 and 1016 except 131 and 899. We suppose that they are not present beacause they didn't have a spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n"
     ]
    }
   ],
   "source": [
    "Yspike=[]\n",
    "for i in range(N):\n",
    "    Yspike.append(times[ids==i])\n",
    "print(len(Yspike))\n",
    "\n",
    "#For now there is only one trial, to change if we want to define trials\n",
    "Yspike=[Yspike]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choices for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of variables\n",
    "dtgrid = 0.004 # discretisation for solving ODEs => maybe try to use one from the data instead\n",
    "xDim = 2 # two latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of model components\n",
    "link = Exp() # exponential link function (to define nonlinearity in likelihood )\n",
    "like = PointProcess(Yspike, link, trLen, dtstep=dtgrid, nLeg=100) # point process likelihood \n",
    "kern = RBF(xDim) # RBF kernel\n",
    "\n",
    "#To initialize the Affine mapping from latent to observations\n",
    "C = 2.*np.random.rand(xDim,N) * np.random.choice([-1,1],size=(xDim,N)) \n",
    "d = 0.1*np.random.randn(1,N)\n",
    "outputMapping = AffineMapping(torch.tensor(C), torch.tensor(d)) # affine output mapping, used inn the forward of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\logiciel\\miniconda\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Choose the inducing points and transition function\n",
    "\n",
    "# generater inducing point locations on a 2D grid (for sparse gp)\n",
    "xmin, xmax = -2., 2.\n",
    "Zs1, Zs2 = torch.meshgrid([torch.linspace(xmin,xmax,5), torch.linspace(xmin,xmax,5)])\n",
    "numZ = 25\n",
    "Zs = torch.cat((Zs1.reshape(-1,1),Zs2.reshape(-1,1)),dim=1)\n",
    "\n",
    "transfunc = SparseGP(kern, Zs) # choose sparse GP as a transition function\n",
    "transfunc.q_mu.data = torch.randn(transfunc.q_mu.size()).type(torch.float64) # random initialisation for inducing point posterior mean\n",
    "\n",
    "# uncomment to instead condition on fixed points\n",
    "#Zs_fx = torch.tensor([0., 0.]).view(-1,xDim)\n",
    "# transition function conditioned on fixed point\n",
    "#transfunc = SparseGP(kern, Zs, Zs_fx) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "\n",
    "# build point procces generative model for continuous time spike-time observations\n",
    "model = PointProcessGPSDEmodel(xDim, transfunc, outputMapping, like, nLeg=100)\n",
    "\n",
    "# assemble inference algorithm\n",
    "inference = GaussMarkovLagrange(xDim, trLen, learningRate=1, dtstep=dtgrid)\n",
    "\n",
    "# create GPSDE model object (final object)\n",
    "myGPSDE = GPSDE(model, inference)\n",
    "\n",
    "# fix inducing points on chosen grid\n",
    "myGPSDE.model.transfunc.Zs.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how long it takes to run the algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estep number of step for inference, mstepiter for learning update, niter for everything"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to do only one step for every part for the all dataset (niter=1,estepIter=1,mstepiter=1) => more than 130 min and it was stopped in inference update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\kernels.py:320: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
      "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
      "X = torch.solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve(A, B) (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:766.)\n",
      "  scaled_diffs, _ = torch.solve(mu_x2_new_diffs.transpose(-1, -2), lengthscales_new_squared.unsqueeze(-3))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m myGPSDE\u001b[39m.\u001b[39;49mvariationalEM(niter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,eStepIter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, mStepIter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:44\u001b[0m, in \u001b[0;36mvariationalEM\u001b[1;34m(self, niter, eStepIter, mStepIter)\u001b[0m\n\u001b[0;32m     41\u001b[0m time0\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(niter):\n\u001b[0;32m     43\u001b[0m     \u001b[39m# run inference across all trials\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minference_update(eStepIter)\n\u001b[0;32m     45\u001b[0m     time1\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     46\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minference time=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(time1\u001b[39m-\u001b[39mtime0))\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:58\u001b[0m, in \u001b[0;36mlearning_update\u001b[1;34m(self, niterM)\u001b[0m\n\u001b[0;32m     55\u001b[0m     time3=time.time()\n\u001b[0;32m     56\u001b[0m     print(\"update time\"+str(time3-time2))\n\u001b[1;32m---> 58\u001b[0m     # print some output and store cost function values\n\u001b[0;32m     59\u001b[0m     self.callback(i, final_ell, final_kld, final_prior_trans)\n\u001b[0;32m     61\u001b[0m # final inference pass\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:264\u001b[0m, in \u001b[0;36mcollectInferenceResults\u001b[1;34m(self, inference)\u001b[0m\n\u001b[0;32m    261\u001b[0m nquad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlike\u001b[39m.\u001b[39mxxLeg\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]  \u001b[39m# number of quadrature nodes\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# create empty tensors to store predicted means and variances across all trials\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m m_qu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(inference\u001b[39m.\u001b[39mnTrials, nquad, \u001b[39m1\u001b[39m, inference\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n\u001b[0;32m    265\u001b[0m S_qu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(inference\u001b[39m.\u001b[39mnTrials, nquad, inference\u001b[39m.\u001b[39mnLatent, inference\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n\u001b[0;32m    266\u001b[0m A_qu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(inference\u001b[39m.\u001b[39mnTrials, nquad, inference\u001b[39m.\u001b[39mnLatent, inference\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\inference.py:164\u001b[0m, in \u001b[0;36mGaussMarkovLagrange.predict_marginals\u001b[1;34m(self, idx, t)\u001b[0m\n\u001b[0;32m    162\u001b[0m S_linp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(T, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnLatent, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnLatent)\u001b[39m.\u001b[39mtype(float_type)\n\u001b[0;32m    163\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m--> 164\u001b[0m     m_linp[i, :, :] \u001b[39m=\u001b[39m linInterp(t[i], m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_grid[idx])\n\u001b[0;32m    165\u001b[0m     S_linp[i, :, :] \u001b[39m=\u001b[39m linInterp(t[i], S, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_grid[idx])\n\u001b[0;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m m_linp, S_linp\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\utils.py:345\u001b[0m, in \u001b[0;36mlinInterp\u001b[1;34m(t, A, t_grid, asnumpy)\u001b[0m\n\u001b[0;32m    343\u001b[0m     Astop \u001b[39m=\u001b[39m A[\u001b[39m0\u001b[39m, ]\n\u001b[0;32m    344\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[1;32m--> 345\u001b[0m \u001b[39melif\u001b[39;00m t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m t_grid\u001b[39m.\u001b[39;49mmax():  \u001b[39m# use only last grid point\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     Astart \u001b[39m=\u001b[39m A[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ]\n\u001b[0;32m    347\u001b[0m     Astop \u001b[39m=\u001b[39m A[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myGPSDE.variationalEM(niter=1,eStepIter=1, mStepIter=1) #estep number of step for inference, mstepiter for learning update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see on a smaller subset of data which part of the algorithm takes longer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 3 neurons with 1 general iteration, 1 iteration for inference and 1 iteration for learning:\n",
    "\n",
    "inference time = 298 s, learning time= 103 s ,update time =0s, Total : 13 min\n",
    "Details inference :\n",
    "forward= 35s, grad= 0.1 s, backward=277s, update= 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "N_neurons=3 #only 3 neurons\n",
    "Y_spike_small=[Yspike[0][:N_neurons]]\n",
    "print(len(Y_spike_small[0]))\n",
    "\n",
    "like_small = PointProcess(Y_spike_small, link, trLen, dtstep=dtgrid, nLeg=100) # point process likelihood \n",
    "C_small = 2.*np.random.rand(xDim,N_neurons) * np.random.choice([-1,1],size=(xDim,N_neurons)) \n",
    "d_small = 0.1*np.random.randn(1,N_neurons)\n",
    "outputMapping_small = AffineMapping(torch.tensor(C_small), torch.tensor(d_small)) # affine output mapping, used inn the forward of the model\n",
    "model_small = PointProcessGPSDEmodel(xDim, transfunc, outputMapping_small, like_small, nLeg=100)\n",
    "# create GPSDE model object (final object)\n",
    "GPSDEsmall = GPSDE(model_small, inference)\n",
    "# fix inducing points on chosen grid\n",
    "GPSDEsmall.model.transfunc.Zs.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeforward34.89074230194092\n",
      "time grad0.1396312713623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\kernels.py:320: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
      "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
      "X = torch.solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve(A, B) (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:766.)\n",
      "  scaled_diffs, _ = torch.solve(mu_x2_new_diffs.transpose(-1, -2), lengthscales_new_squared.unsqueeze(-3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time backward277.1971700191498\n",
      "time update infer1.6904823780059814\n",
      "inference time=313.92301058769226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\utils.py:140: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A).transpose(-2, -1).conj().\n",
      "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ..\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1285.)\n",
      "  L = torch.cholesky(M)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning time=86.98113107681274\n",
      "update time0.0009975433349609375\n",
      "-------------------------------------------------------\n",
      "iter   objective    log-like      kl-div     f-prior\n",
      "-------------------------------------------------------\n",
      "   0    3911.546   -2675.130     343.637    -892.778\n",
      "timeforward34.368138551712036\n",
      "time grad0.13862824440002441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m GPSDEsmall\u001b[39m.\u001b[39;49mvariationalEM(niter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,eStepIter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, mStepIter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:62\u001b[0m, in \u001b[0;36mGPSDE.variationalEM\u001b[1;34m(self, niter, eStepIter, mStepIter)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(i, final_ell, final_kld, final_prior_trans)\n\u001b[0;32m     61\u001b[0m \u001b[39m# final inference pass\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference_update(eStepIter)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\models.py:82\u001b[0m, in \u001b[0;36mGPSDE.inference_update\u001b[1;34m(self, niterE)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference_update\u001b[39m(\u001b[39mself\u001b[39m, niterE):\n\u001b[1;32m---> 82\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference\u001b[39m.\u001b[39;49mrun_inference(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, niterE)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\inference.py:234\u001b[0m, in \u001b[0;36mGaussMarkovLagrange.run_inference\u001b[1;34m(self, model, niter)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnTrials):\n\u001b[0;32m    231\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(niter):\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m         \u001b[39m# run inference loops for ith trial\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m         convergence_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_inference_single(model, idx)\n\u001b[0;32m    236\u001b[0m         \u001b[39m# break loop if trial has converged\u001b[39;00m\n\u001b[0;32m    237\u001b[0m         \u001b[39mif\u001b[39;00m convergence_flag \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\inference.py:214\u001b[0m, in \u001b[0;36mGaussMarkovLagrange.run_inference_single\u001b[1;34m(self, model, idx)\u001b[0m\n\u001b[0;32m    211\u001b[0m timegrad\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    212\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtime grad\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(timegrad\u001b[39m-\u001b[39mtimeforward))\n\u001b[1;32m--> 214\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolveBackward_LagrangeMultipliers(model, m, S, dLdm, dLdm_jump, dLdS, dLdS_jump, idx)\n\u001b[0;32m    215\u001b[0m timebackward\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    216\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtime backward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(timebackward\u001b[39m-\u001b[39mtimegrad))\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\inference.py:37\u001b[0m, in \u001b[0;36mGaussMarkovLagrange.solveBackward_LagrangeMultipliers\u001b[1;34m(self, model, m, S, dLdm, dLdm_jump, dLdS, dLdS_jump, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolveBackward_LagrangeMultipliers\u001b[39m(\u001b[39mself\u001b[39m, model, m, S, dLdm, dLdm_jump, dLdS, dLdS_jump, idx):\n\u001b[0;32m     33\u001b[0m     \u001b[39m# solve Lagrange multipliers and incorporate jump conditions due to observations\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39m# timepts are the time points corresponding to the discretisation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m     \u001b[39m# get cost function gradients at given discretization\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     dEdm, dEdS \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_KullbackLeibler_grad(model, m, S, idx)\n\u001b[0;32m     39\u001b[0m     \u001b[39m# set final condition\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     Psi_new \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPsi[idx])\u001b[39m.\u001b[39mtype(float_type)  \u001b[39m# T x D x D\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\inference.py:66\u001b[0m, in \u001b[0;36mGaussMarkovLagrange.get_KullbackLeibler_grad\u001b[1;34m(self, model, m, S, idx)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_KullbackLeibler_grad\u001b[39m(\u001b[39mself\u001b[39m, model, m, S, idx):\n\u001b[0;32m     63\u001b[0m     \u001b[39m# gradients for Kullback leibler divergence\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# gradients wrt m are (R x 1 x 1) x (1 x K)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 66\u001b[0m         dEdm_grid \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m model\u001b[39m.\u001b[39;49mtransfunc\u001b[39m.\u001b[39;49mdffdm(m, S) \\\n\u001b[0;32m     67\u001b[0m             \u001b[39m-\u001b[39m (model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mdfdm(m, S) \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_grid[idx]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \\\n\u001b[0;32m     68\u001b[0m             \u001b[39m+\u001b[39m m\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx]\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx])\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \\\n\u001b[0;32m     69\u001b[0m             \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_grid[idx]\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx])\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \\\n\u001b[0;32m     70\u001b[0m             \u001b[39m+\u001b[39m model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mf(m, S)\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx])\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \\\n\u001b[0;32m     71\u001b[0m             \u001b[39m+\u001b[39m (model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mdfdm(m, S) \u001b[39m*\u001b[39m m\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx]\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \\\n\u001b[0;32m     72\u001b[0m             \u001b[39m+\u001b[39m (model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mddfdxdm(m, S) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx]\u001b[39m.\u001b[39mmatmul(S)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m         \u001b[39m# gradients wrt S are  (R x 1 x 1) x (K x K)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m         \u001b[39m# part of gradient that is already symmetrised (since grads come from transition function, which expects proper gradients)\u001b[39;00m\n\u001b[0;32m     76\u001b[0m         dEdS_grid_sym \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mdffdS(m, S) \\\n\u001b[0;32m     77\u001b[0m             \u001b[39m-\u001b[39m (model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mdfdS(m, S) \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_grid[idx]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \\\n\u001b[0;32m     78\u001b[0m             \u001b[39m+\u001b[39m (model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mdfdS(m, S) \u001b[39m*\u001b[39m m\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx]\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \\\n\u001b[0;32m     79\u001b[0m             \u001b[39m+\u001b[39m (model\u001b[39m.\u001b[39mtransfunc\u001b[39m.\u001b[39mddfdxdS(m, S) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_grid[idx]\u001b[39m.\u001b[39mmatmul(S)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\gp-sde\\demos\\..\\transition.py:555\u001b[0m, in \u001b[0;36mSparseGP.dffdm\u001b[1;34m(self, m, S)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39m# trace ( Kzz^{-1} sum(Su) Kzz^{-1} Kzxxz ) = vec ( sum(Su) Kzz^{-1}) * vec( Kzz^{-1} Kzxxz )\u001b[39;00m\n\u001b[0;32m    554\u001b[0m term2 \u001b[39m=\u001b[39m (Kzz_inv_q_sigmas\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m) \u001b[39m*\u001b[39m Kzz_inv_Kzxxz)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m--> 555\u001b[0m term3 \u001b[39m=\u001b[39m (Kzxxz\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mmatmul(unsqueeze_as(Kzz_inv_U, Kzxxz, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)) \u001b[39m*\u001b[39m unsqueeze_as(Kzz_inv_U, Kzxxz, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    556\u001b[0m dff \u001b[39m=\u001b[39m term1 \u001b[39m+\u001b[39m term2 \u001b[39m+\u001b[39m term3\n\u001b[0;32m    558\u001b[0m \u001b[39m# term4 = Kzz_inv_Kzxxz.sum(-2, keepdim=True).sum(-1, keepdim=True).permute(0, 3, 4, 1, 2)\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m# dff = term2\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GPSDEsmall.variationalEM(niter=1,eStepIter=1, mStepIter=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
