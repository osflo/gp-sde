c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\kernels.py:320: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  ..\aten\src\ATen\native\BatchLinearAlgebra.cpp:766.)
  scaled_diffs, _ = torch.solve(mu_x2_new_diffs.transpose(-1, -2), lengthscales_new_squared.unsqueeze(-3))
inference time=778.0713171958923
learning time=192.22189283370972
update time0.0
-------------------------------------------------------
iter   objective    log-like      kl-div     f-prior
-------------------------------------------------------
   0  277512.263 -239903.505    1095.300  -36513.458
inference time=1747.6515381336212
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[9], line 1
----> 1 myGPSDE.variationalEM(niter=10,eStepIter=10, mStepIter=10)

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\models.py:49, in GPSDE.variationalEM(self, niter, eStepIter, mStepIter)
     46 print("inference time="+str(time1-time0))
     48 # run learning of transferfunction and other model parameters, hyperparameters
---> 49 final_ell, final_kld, final_prior_trans, _ = self.learning_update(mStepIter)
     50 time2=time.time()
     51 print('learning time='+str(time2-time1))

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\models.py:73, in GPSDE.learning_update(self, niterM)
     70 self.model.closedFormUpdates(*inputs)
     72 # train model parameters
---> 73 final_ell, final_kld, final_prior_trans, final_prior_map = train_model(self.model, inputs, niterM)
     74 # refresh any values that are stored throughout inference but updated during learning
     75 self.model.transfunc.refresh_stored_values()

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\utils.py:428, in train_model(model, inputs, maxiter)
    426 optimizer = torch.optim.LBFGS(filter(lambda p: p.requires_grad, model.parameters()), max_iter=maxiter)
    427 # Forward pass: Compute predicted mean and varince by passing input to the model
--> 428 ell, kld, prior_trans, prior_map = model(*inputs)
    429 loss = neg_variational_free_energy(ell, kld, prior_trans, prior_map)
    431 def closure():

File c:\Users\fosmo\anaconda3\lib\site-packages\torch\nn\modules\module.py:1102, in Module._call_impl(self, *input, **kwargs)
   1098 # If we don't have any hooks, we want to skip the rest of the logic in
   1099 # this function, and just call forward.
   1100 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1102     return forward_call(*input, **kwargs)
   1103 # Do not call functions when jit is used
   1104 full_backward_hooks, non_full_backward_hooks = [], []

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\models.py:295, in PointProcessGPSDEmodel.forward(self, m_sp, S_sp, m_qu, S_qu, A_qu, b_qu)
    292 mu_qu, cov_qu, prior_map = self.outputMapping(m_qu, S_qu)  # R x T x 1 x K and R x T x K x K
    294 # compute KL divergence between approx post and prior SDE
--> 295 fx, ffx, dfdx, prior_trans = self.transfunc(m_qu, S_qu)
    296 kld = self.KLdiv(fx, ffx, dfdx, m_qu, S_qu, A_qu, b_qu)
    298 # compute expected log likelihood

File c:\Users\fosmo\anaconda3\lib\site-packages\torch\nn\modules\module.py:1102, in Module._call_impl(self, *input, **kwargs)
   1098 # If we don't have any hooks, we want to skip the rest of the logic in
   1099 # this function, and just call forward.
   1100 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1102     return forward_call(*input, **kwargs)
   1103 # Do not call functions when jit is used
   1104 full_backward_hooks, non_full_backward_hooks = [], []

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\transition.py:125, in TransitionFunction.forward(self, m, S)
    123 ff = self.ff(m, S)
    124 dfdx = self.dfdx(m, S)
--> 125 prior = self.log_prior_distribution()
    126 return f, ff, dfdx, prior

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\transition.py:318, in SparseGP.log_prior_distribution(self)
    316 def log_prior_distribution(self):
    317     # for cost function computation
--> 318     return - self.kullback_leibler()

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\transition.py:602, in SparseGP.kullback_leibler(self)
    599 # logdet(Kzz) - logdet(q_sigma) - k
    600 tt3 = self.xDim * apply_along_axis(logdet, Kzz, dim=0).sum()
--> 602 tt4 = - apply_along_axis(logdet, q_sigma, dim=0).sum() - self.xDim * numZ
    604 # sum everything up
    605 kld = torch.sum(0.5 * (tt1 + tt2 + tt3 + tt4))

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\utils.py:118, in apply_along_axis(func, M, dim)
    117 def apply_along_axis(func, M, dim):
--> 118     tList = [func(m) for m in torch.unbind(M, dim)]
    119     res = torch.stack(tList, dim).to(device=M.device)
    120     return res

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\utils.py:118, in (.0)
    117 def apply_along_axis(func, M, dim):
--> 118     tList = [func(m) for m in torch.unbind(M, dim)]
    119     res = torch.stack(tList, dim).to(device=M.device)
    120     return res

File c:\Users\fosmo\OneDrive\Documents\GitHub\gp-sde\demos\..\utils.py:141, in logdet(M)
    138 def logdet(M):
    139     #L = torch.potrf(M, upper=False)
    140     M=M+5e-3 #trying to make spd
--> 141     L = torch.linalg.cholesky(M)
    142     return 2 * torch.diag(L).log().sum().unsqueeze(-1).unsqueeze(-1)

RuntimeError: torch.linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).





